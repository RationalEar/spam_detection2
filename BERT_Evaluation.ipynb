{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Evaluation\n",
    "This notebook demonstrates the comprehensive evaluation features of the BERT model for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.349856Z",
     "start_time": "2025-05-26T12:23:59.845467Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from models.bert import SpamBERT\n",
    "from utils.functions import load_glove_embeddings, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.509077Z",
     "start_time": "2025-05-26T12:24:06.503705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/PycharmProjects/spam-detection-data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = './'\n",
    "DATA_PATH = os.path.abspath(os.path.join(ROOT_PATH, '../spam-detection-data/'))\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.561677Z",
     "start_time": "2025-05-26T12:24:06.557749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/PycharmProjects/spam-detection-data/data/raw/glove.6B/glove.6B.300d.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOVE_PATH = os.path.join(DATA_PATH, 'data/raw/glove.6B/glove.6B.300d.txt')\n",
    "GLOVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.620178Z",
     "start_time": "2025-05-26T12:24:06.604230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_pickle(DATA_PATH + '/data/processed/train.pkl')\n",
    "test_df = pd.read_pickle(DATA_PATH + '/data/processed/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.663097Z",
     "start_time": "2025-05-26T12:24:06.658286Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:06.713976Z",
     "start_time": "2025-05-26T12:24:06.709625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:07.196866Z",
     "start_time": "2025-05-26T12:24:06.768212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build vocabulary and load embeddings\n",
    "set_seed(42)\n",
    "word2idx, idx2word = build_vocab(train_df['text'])\n",
    "embedding_dim = 300\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:35.207906Z",
     "start_time": "2025-05-26T12:24:07.320682Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embeddings = load_glove_embeddings(GLOVE_PATH, word2idx, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:24:38.806612Z",
     "start_time": "2025-05-26T12:24:35.293888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpamBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "model = SpamBERT(bert_model_name=bert_model_name)\n",
    "model.load(os.path.join(DATA_PATH, 'trained_models', 'spam_bert.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comprehensive Performance Metrics\n",
    "\n",
    "Here we implement the evaluation metrics from Section 3.5.1 of the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:28:06.130957Z",
     "start_time": "2025-05-26T12:24:38.834984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cdce0f09e74568ae0c4b726b7b27cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9793\n",
      "precision: 0.9494\n",
      "recall: 0.9868\n",
      "f1: 0.9677\n",
      "auc_roc: 0.9972\n",
      "spam_catch_rate: 0.9868\n",
      "ham_preservation_rate: 0.9759\n"
     ]
    }
   ],
   "source": [
    "from utils.bert_evaluation import evaluate_model, compute_metrics\n",
    "\n",
    "# Run evaluation\n",
    "predictions, labels = evaluate_model(model, test_df, tokenizer)\n",
    "metrics = compute_metrics(predictions, labels)\n",
    "\n",
    "# Display results\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explainability Analysis\n",
    "\n",
    "Implementing explainability metrics from Section 3.5.2 of the thesis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T12:28:07.272905Z",
     "start_time": "2025-05-26T12:28:06.167840Z"
    }
   },
   "source": [
    "from utils.bert_evaluation import analyze_explanations, visualize_attention\n",
    "\n",
    "# Analyze a sample email\n",
    "sample_text = test_df['text'].iloc[0]\n",
    "explanation_data, prob = analyze_explanations(model, sample_text, tokenizer)\n",
    "\n",
    "# Visualize attention for layer 12\n",
    "if 'layer_12' in explanation_data:\n",
    "    attention_weights = explanation_data['layer_12'][0].cpu().numpy()\n",
    "    visualize_attention(sample_text, attention_weights, tokenizer)\n",
    "    \n",
    "print(f\"Prediction probability: {prob.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SpamBERT.forward_for_ig() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Analyze a sample email\u001B[39;00m\n\u001B[1;32m      4\u001B[0m sample_text \u001B[38;5;241m=\u001B[39m test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m explanation_data, prob \u001B[38;5;241m=\u001B[39m \u001B[43manalyze_explanations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Visualize attention for layer 12\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer_12\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m explanation_data:\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/utils/bert_evaluation.py:61\u001B[0m, in \u001B[0;36manalyze_explanations\u001B[0;34m(model, text, tokenizer, device)\u001B[0m\n\u001B[1;32m     58\u001B[0m probs, attention_data \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, return_attentions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# Get integrated gradients\u001B[39;00m\n\u001B[0;32m---> 61\u001B[0m attributions, delta \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_integrated_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Compute explanation metrics\u001B[39;00m\n\u001B[1;32m     64\u001B[0m explanation_data \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattributions\u001B[39m\u001B[38;5;124m'\u001B[39m: attributions,\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattention_data\n\u001B[1;32m     67\u001B[0m }\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/models/bert.py:107\u001B[0m, in \u001B[0;36mSpamBERT.compute_integrated_gradients\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, n_steps)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;124;03mCompute integrated gradients for input attribution\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m    tuple: (attributions, delta)\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m lig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_layer_integrated_gradients()\n\u001B[0;32m--> 107\u001B[0m attributions, delta \u001B[38;5;241m=\u001B[39m \u001B[43mlig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattribute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_convergence_delta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    112\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m attributions, delta\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/log/dummy_log.py:39\u001B[0m, in \u001B[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:521\u001B[0m, in \u001B[0;36mLayerIntegratedGradients.attribute\u001B[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_func, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevice_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 521\u001B[0m inputs_layer \u001B[38;5;241m=\u001B[39m \u001B[43m_forward_layer_eval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43minps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    529\u001B[0m input_layer_list: List[Tuple[Tensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# if we have one output\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/_utils/gradient.py:210\u001B[0m, in \u001B[0;36m_forward_layer_eval\u001B[0;34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_forward_layer_eval\u001B[39m(\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m     forward_fn: Callable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    208\u001B[0m     grad_enabled: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple[Tensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[Tuple[Tensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[0;32m--> 210\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_forward_layer_eval_with_neuron_grads\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# pyre-fixme[6]: For 3rd argument expected `Module` but got\u001B[39;49;00m\n\u001B[1;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#  `ModuleOrModuleList`.\u001B[39;49;00m\n\u001B[1;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_neuron_selector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_enabled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_enabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/_utils/gradient.py:506\u001B[0m, in \u001B[0;36m_forward_layer_eval_with_neuron_grads\u001B[0;34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001B[0m\n\u001B[1;32m    503\u001B[0m grad_enabled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m gradient_neuron_selector \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m grad_enabled\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_grad_enabled(grad_enabled):\n\u001B[0;32m--> 506\u001B[0m     saved_layer \u001B[38;5;241m=\u001B[39m \u001B[43m_forward_layer_distributed_eval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattribute_to_layer_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    513\u001B[0m device_ids \u001B[38;5;241m=\u001B[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001B[1;32m    514\u001B[0m \u001B[38;5;66;03m# Identifies correct device ordering based on device ids.\u001B[39;00m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;66;03m# key_list is a list of devices in appropriate ordering for concatenation.\u001B[39;00m\n\u001B[1;32m    516\u001B[0m \u001B[38;5;66;03m# If only one key exists (standard model), key list simply has one element.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/_utils/gradient.py:339\u001B[0m, in \u001B[0;36m_forward_layer_distributed_eval\u001B[0;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001B[0m\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m         all_hooks\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    337\u001B[0m             single_layer\u001B[38;5;241m.\u001B[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001B[1;32m    338\u001B[0m         )\n\u001B[0;32m--> 339\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43m_run_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_ind\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# _run_forward may return future of Tensor,\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# but we don't support it here now\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;66;03m# And it will fail before here.\u001B[39;00m\n\u001B[1;32m    348\u001B[0m output \u001B[38;5;241m=\u001B[39m cast(Tensor, output)\n",
      "File \u001B[0;32m~/PycharmProjects/spam_detection2/venv/lib/python3.10/site-packages/captum/_utils/common.py:588\u001B[0m, in \u001B[0;36m_run_forward\u001B[0;34m(forward_func, inputs, target, additional_forward_args)\u001B[0m\n\u001B[1;32m    585\u001B[0m inputs \u001B[38;5;241m=\u001B[39m _format_inputs(inputs)\n\u001B[1;32m    586\u001B[0m additional_forward_args \u001B[38;5;241m=\u001B[39m _format_additional_forward_args(additional_forward_args)\n\u001B[0;32m--> 588\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mforward_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001B[39;49;00m\n\u001B[1;32m    591\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001B[39;49;00m\n\u001B[1;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    596\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, torch\u001B[38;5;241m.\u001B[39mfutures\u001B[38;5;241m.\u001B[39mFuture):\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\u001B[38;5;241m.\u001B[39mthen(\u001B[38;5;28;01mlambda\u001B[39;00m x: _select_targets(x\u001B[38;5;241m.\u001B[39mvalue(), target))\n",
      "\u001B[0;31mTypeError\u001B[0m: SpamBERT.forward_for_ig() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adversarial Robustness Analysis\n",
    "\n",
    "Implementing adversarial robustness metrics from Section 3.5.4 of the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bert_evaluation import evaluate_adversarial_robustness\n",
    "\n",
    "# Evaluate adversarial robustness\n",
    "robustness_metrics = evaluate_adversarial_robustness(model, test_df, tokenizer)\n",
    "\n",
    "for metric, value in robustness_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
